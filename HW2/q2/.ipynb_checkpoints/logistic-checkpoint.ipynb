{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import sigmoid\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def logistic_predict(weights, data):\n",
    "    \"\"\" Compute the probabilities predicted by the logistic classifier.\n",
    "\n",
    "    Note: N is the number of examples\n",
    "          M is the number of features per example\n",
    "\n",
    "    :param weights: A vector of weights with dimension (M + 1) x 1, where\n",
    "    the last element corresponds to the bias (intercept).\n",
    "    :param data: A matrix with dimension N x M, where each row corresponds to\n",
    "    one data point.\n",
    "    :return: A vector of probabilities with dimension N x 1, which is the output\n",
    "    to the classifier.\n",
    "    \"\"\"\n",
    "    #####################################################################\n",
    "    # TODO:                                                             #\n",
    "    # Given the weights and bias, compute the probabilities predicted   #\n",
    "    # by the logistic classifier.                                       #\n",
    "    #####################################################################\n",
    "    \n",
    "    data = np.hstack((data, np.ones((len(data), 1))))\n",
    "    y = sigmoid(np.dot(data, weights))\n",
    "    \n",
    "    #####################################################################\n",
    "    #                       END OF YOUR CODE                            #\n",
    "    #####################################################################\n",
    "    return y\n",
    "\n",
    "def evaluate(targets, y):\n",
    "    \"\"\" Compute evaluation metrics.\n",
    "\n",
    "    Note: N is the number of examples\n",
    "          M is the number of features per example\n",
    "\n",
    "    :param targets: A vector of targets with dimension N x 1.\n",
    "    :param y: A vector of probabilities with dimension N x 1.\n",
    "    :return: A tuple (ce, frac_correct)\n",
    "        WHERE\n",
    "        ce: (float) Averaged cross entropy\n",
    "        frac_correct: (float) Fraction of inputs classified correctly\n",
    "    \"\"\"\n",
    "    #####################################################################\n",
    "    # TODO:                                                             #\n",
    "    # Given targets and probabilities predicted by the classifier,      #\n",
    "    # return cross entropy and the fraction of inputs classified        #\n",
    "    # correctly.                                                        #\n",
    "    #####################################################################\n",
    "    \n",
    "    loss = -1 * targets * np.log(y) - (1-targets) * np.log(1-y)\n",
    "    ce = np.mean(loss)\n",
    "    frac_correct = np.sum(np.equal(targets, np.around(y))) / float(len(y))\n",
    "    \n",
    "    #####################################################################\n",
    "    #                       END OF YOUR CODE                            #\n",
    "    #####################################################################\n",
    "    return ce, frac_correct\n",
    "\n",
    "def logistic(weights, data, targets, hyperparameters):\n",
    "    \"\"\" Calculate the cost and its derivatives with respect to weights.\n",
    "    Also return the predictions.\n",
    "\n",
    "    Note: N is the number of examples\n",
    "          M is the number of features per example\n",
    "\n",
    "    :param weights: A vector of weights with dimension (M + 1) x 1, where\n",
    "    the last element corresponds to the bias (intercept).\n",
    "    :param data: A matrix with dimension N x M, where each row corresponds to\n",
    "    one data point.\n",
    "    :param targets: A vector of targets with dimension N x 1.\n",
    "    :param hyperparameters: The hyperparameter dictionary.\n",
    "    :returns: A tuple (f, df, y)\n",
    "        WHERE\n",
    "        f: The average of the loss over all data points.\n",
    "           This is the objective that we want to minimize.\n",
    "        df: (M + 1) x 1 vector of derivative of f w.r.t. weights.\n",
    "        y: N x 1 vector of probabilities.\n",
    "    \"\"\"\n",
    "    y = logistic_predict(weights, data)\n",
    "\n",
    "    #####################################################################\n",
    "    # TODO:                                                             #\n",
    "    # Given weights and data, return the averaged loss over all data    #\n",
    "    # points, gradient of parameters, and the probabilities given by    #\n",
    "    # logistic regression.                                              #\n",
    "    #####################################################################\n",
    "    \n",
    "    data = np.hstack((data, np.ones((len(data), 1))))\n",
    "    f = evaluate(targets, y)[0]\n",
    "    df = np.dot(np.transpose(data), y - targets) / len(data)\n",
    "    \n",
    "    #####################################################################\n",
    "    #                       END OF YOUR CODE                            #\n",
    "    #####################################################################\n",
    "    return f, df, y\n",
    "\n",
    "def logistic_pen(weights, data, targets, hyperparameters):\n",
    "    \"\"\" Calculate the cost of penalized logistic regression and its derivatives\n",
    "    with respect to weights. Also return the predictions.\n",
    "\n",
    "    Note: N is the number of examples\n",
    "          M is the number of features per example\n",
    "\n",
    "    :param weights: A vector of weights with dimension (M + 1) x 1, where\n",
    "    the last element corresponds to the bias (intercept).\n",
    "    :param data: A matrix with dimension N x M, where each row corresponds to\n",
    "    one data point.\n",
    "    :param targets: A vector of targets with dimension N x 1.\n",
    "    :param hyperparameters: The hyperparameter dictionary.\n",
    "    :returns: A tuple (f, df, y)\n",
    "        WHERE\n",
    "        f: The average of the loss over all data points, plus a penalty term.\n",
    "           This is the objective that we want to minimize.\n",
    "        df: (M+1) x 1 vector of derivative of f w.r.t. weights.\n",
    "        y: N x 1 vector of probabilities.\n",
    "    \"\"\"\n",
    "    y = logistic_predict(weights, data)\n",
    "\n",
    "    #####################################################################\n",
    "    # TODO:                                                             #\n",
    "    # Given weights and data, return the averaged loss over all data    #\n",
    "    # points (plus a penalty term), gradient of parameters, and the     #\n",
    "    # probabilities given by penalized logistic regression.             #\n",
    "    #####################################################################\n",
    "    \n",
    "    data = np.hstack((data, np.ones((len(data), 1))))\n",
    "    weights[-1] = [0.0]\n",
    "    f = evaluate(targets, y)[0] + hyperparameters[\"weight_regularization\"] / 2.0 * np.linalg.norm(weights) ** 2\n",
    "    df = np.matmul(np.transpose(data), y - targets) / len(data) + np.multiply(weights, hyperparameters[\"weight_regularization\"])\n",
    "    \n",
    "    #####################################################################\n",
    "    #                       END OF YOUR CODE                            #\n",
    "    #####################################################################\n",
    "    return f, df, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
